# 模型切换功能实现计划

## 📋 项目概述

在 ai-coder 聊天输入框左下角添加模型下拉选择器（参考 Cursor），支持切换 Claude 的 sonnet、opus、haiku 模型，并确保模型参数正确传递到 llms 服务。

## 🔍 架构分析

### 数据流

```
前端 (sonnet/opus/haiku) 
  ↓
ai-coder 后端 (映射为完整模型名称)
  ↓
Claude SDK client.set_model()
  ↓
Claude Code CLI (HTTP 请求体传递模型名称)
  ↓
llms 服务 (自动路由到对应的 provider/model)
```

### 关键发现

#### Claude SDK 支持
- ✅ `client.set_model()` 动态切换模型（`client.py` 第 230-252 行）
- ⚠️ 没有内部优化，每次调用都会发送控制协议请求（`query.py` 第 504-511 行）
- 💡 需要在应用层记录当前模型状态，避免重复切换

#### llms 服务支持
- ✅ 从 `req.body.model` 读取模型名称（`llms/src/server.ts` 第 143 行）
- ✅ 支持 `provider,model` 格式和纯模型名称
- ✅ 自动解析和路由，**无需修改 llms 代码**

#### Claude Code CLI 限制
- ⚠️ 交互界面只显示 3 个 Claude 模型（UI 限制）
- ✅ 但支持通过 `/model` 命令或 `--model` 参数传入任意模型名称
- ✅ 支持 `provider,model` 格式（参考 claude-code-router）

## 📝 实现步骤

### 1. 前端实现

**文件**: 
- `ai-coder/frontend/src/index.html`
- `ai-coder/frontend/src/js/features.js`
- `ai-coder/frontend/src/css/app.css`

**任务清单**:
- [ ] 在输入框容器左下角添加模型选择下拉菜单
- [ ] 支持三个选项：sonnet、opus、haiku
- [ ] 默认选择 sonnet
- [ ] 将选中的模型保存到 localStorage
- [ ] 在 `sendChat()` 函数中添加 `model` 参数
- [ ] 下拉菜单样式参考 Cursor 的设计风格

**实现细节**:
```javascript
// 在 input-section 容器中添加模型选择器
<div class="model-selector">
  <select id="modelSelector">
    <option value="sonnet">Sonnet</option>
    <option value="opus">Opus</option>
    <option value="haiku">Haiku</option>
  </select>
</div>

// 在 sendChat() 中携带模型参数
const model = document.getElementById('modelSelector').value;
const data = await sendChat(message, params, abortController.signal, model);
```

### 2. 后端 API 修改

**文件**: 
- `ai-coder/backend/src/common/model.py`
- `ai-coder/backend/src/api/api_chat.py`
- `ai-coder/backend/src/manager/client_manager.py`

**任务清单**:
- [ ] 在 `ChatRequest` 中添加可选的 `model` 字段
- [ ] 在 `ClientInfo` 中添加 `current_model` 字段记录当前模型
- [ ] 实现模型名称映射函数
- [ ] 在 `chat()` 接口中接收模型参数
- [ ] 实现模型切换优化逻辑（只在模型变化时调用 `set_model()`）

**模型名称映射**:
```python
MODEL_MAPPING = {
    "sonnet": "claude-sonnet-4-5",
    "opus": "claude-opus-4-20250514",
    "haiku": "claude-haiku-4-20250514",
}

def map_model_name(model: str) -> str:
    """将简化的模型名称映射为完整的 Claude 模型名称"""
    return MODEL_MAPPING.get(model.lower(), model)
```

**优化逻辑**:
```python
# 在 chat() 接口中
if request.model:
    mapped_model = map_model_name(request.model)
    
    # 只在模型变化时才切换
    if mapped_model != client_info.current_model:
        await client.set_model(mapped_model)
        client_info.current_model = request.model
```

### 3. llms 服务适配

**文件**: 无需修改 llms 代码

**说明**:
- ✅ llms 服务已支持从请求体读取 `model` 字段
- ✅ 会自动解析和路由模型名称
- ✅ Claude Code CLI 会自动将模型名称传递给 llms 服务

**验证步骤**:
- [ ] 确认 llms 服务配置了对应的模型路由
- [ ] 测试模型切换功能是否正常工作
- [ ] 验证模型参数是否正确传递到 llms 服务

## 🔧 技术细节

### 模型切换优化

**问题**: SDK 的 `set_model()` 没有内部优化，每次调用都会发送控制协议请求

**解决方案**:
1. 在 `ClientInfo` 中记录 `current_model` 字段
2. 在 `chat()` 接口中比较请求的模型与当前模型
3. 只在模型不同时才调用 `client.set_model()`

### 模型传递流程

1. **前端** → 发送 `{message: "...", model: "sonnet"}`
2. **后端** → 映射为 `claude-sonnet-4-5`，调用 `client.set_model("claude-sonnet-4-5")`
3. **Claude Code CLI** → 在 HTTP 请求体中传递 `model: "claude-sonnet-4-5"`
4. **llms 服务** → 从 `req.body.model` 读取，自动路由到对应的 provider/model

### 代码修改位置

#### 前端修改
1. **HTML 结构** (`index.html`):
   - 在 `input-section` 容器中添加模型选择器
   - 位置：输入框左下角

2. **JavaScript 逻辑** (`features.js`):
   - 修改 `sendChat()` 函数，添加 `model` 参数
   - 添加模型选择器的 localStorage 持久化逻辑
   - 添加模型选择器的事件监听

3. **CSS 样式** (`app.css`):
   - 添加模型选择器的样式
   - 参考 Cursor 的设计风格

#### 后端修改
1. **数据模型** (`model.py`):
   - `ChatRequest`: 添加 `model: str | None = None`
   - `ClientInfo`: 添加 `current_model: str | None = None`

2. **API 接口** (`api_chat.py`):
   - 在 `chat()` 函数中接收 `request.model`
   - 实现模型切换逻辑

3. **Client 管理** (`client_manager.py`):
   - 添加模型名称映射函数
   - 更新 `ClientInfo` 的 `current_model` 字段

## 📊 待办事项清单

### 前端开发
- [ ] 设计并实现模型选择下拉菜单 UI
- [ ] 实现模型选择的 localStorage 持久化
- [ ] 修改 `sendChat()` 函数，添加模型参数
- [ ] 添加模型选择器的事件处理逻辑
- [ ] 测试模型选择功能

### 后端开发
- [ ] 修改 `ChatRequest` 模型，添加 `model` 字段
- [ ] 修改 `ClientInfo` 类，添加 `current_model` 字段
- [ ] 实现模型名称映射函数
- [ ] 修改 `chat()` 接口，实现模型切换逻辑
- [ ] 实现模型切换优化（避免重复切换）
- [ ] 添加日志记录模型切换操作
- [ ] 测试模型切换功能

### 测试验证
- [ ] 测试前端模型选择器功能
- [ ] 测试模型参数是否正确传递到后端
- [ ] 测试模型切换是否正常工作
- [ ] 测试模型切换优化是否生效
- [ ] 测试 llms 服务是否正确接收模型参数
- [ ] 测试不同模型的实际调用效果

## ⚠️ 注意事项

1. **模型切换只影响后续消息**，不影响已发送的消息
2. **前端需要处理模型选择的状态持久化**（localStorage）
3. **下拉菜单样式参考 Cursor 的设计风格**
4. **确保 llms 服务配置了对应的模型路由**（默认已配置）
5. **模型切换需要优化**，避免重复切换相同模型
6. **错误处理**：如果模型切换失败，需要回退到默认模型或显示错误提示

## 🚀 未来扩展

### 短期扩展
- 添加模型切换的视觉反馈（当前使用的模型高亮显示）
- 添加模型切换的快捷键支持
- 添加模型切换的历史记录

### 长期扩展
- 如果需要在 llms 服务中配置其他 provider（如 OpenAI），可以添加更多选项
- 使用 `provider,model` 格式（如 `openai,gpt-4o`）
- 前端下拉菜单可以动态加载配置的模型列表
- 支持自定义模型配置

## 📚 参考文档

- Claude SDK 文档：`claude-code-sdk-python/src/claude_agent_sdk/client.py`
- llms 服务实现：`llms/src/server.ts`, `llms/src/api/routes.ts`
- claude-code-router 参考：`claude-code-router/src/utils/router.ts`

## 📅 预计工作量

- **前端开发**: 2-3 小时
- **后端开发**: 2-3 小时
- **测试验证**: 1-2 小时
- **总计**: 5-8 小时

---

**创建时间**: 2025-01-XX
**状态**: 待开始
**优先级**: 中

